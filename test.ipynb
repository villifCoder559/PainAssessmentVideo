{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoMAEv2 model availables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train\n",
    "\n",
    "| Model | Config | Dataset | \n",
    "| :---: | :----  | :-----: | \n",
    "| ViT-giant | vit_g_hybrid_pt_1200e | UnlabeledHybrid | \n",
    "\n",
    "### Fine-tune\n",
    "| Model | Config | Dataset | Pre-train | Post-pre-train |\n",
    "| :---: | :----  | :-----: | :-------: | :------------: |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_ft | K710 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k400_ft | K400 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k400_ft | K400 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k600_ft | K600 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_ssv2_ft | SSv2 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_ucf101_ft | UCF101 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_hmdb51_ft | HMDB51 | UnlabeledHybrid | K710 |\n",
    "\n",
    "### Distillation from giant\n",
    "|  Model  | Dataset | Teacher Model  |\n",
    "| :-----: | :-----: | :-----------: |\n",
    "| ViT-small | K710 | vit_g_hybrid_pt_1200e_k710_ft |\n",
    "| ViT-base | K710 | vit_g_hybrid_pt_1200e_k710_ft | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model details\n",
    "\n",
    "|  model  | frame channels | frame sampling | frame size (H,W) | tubelet size | patch size | emb dim | output tensor | mem(GB) |\n",
    "| :-----: | :-----: | :-----------: | :-----: | :-----: | :-----------: | :-----: | :-----: |:----|\n",
    "| giant | 3 | 16 | (224,224) | 2 | (14,14) | 1408 | [8,16,16,1408] | 4.0 |\n",
    "| base | 3 | 16 | (224,224) | 2 | (16,16) | 768 | [8,14,14,768] | 0.4|\n",
    "| small | 3 | 16 | (224,224) | 2 | (16,16) | 384 | [8,14,14,1408] | 0.09|\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE 1\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8,(8*1*1*768)] =>\n",
    "    => 2 GRU((6144,512)|dropout(0.5)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "\n",
    "CASE 2\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8*8,(1*1*768)] =>\n",
    "    => 2 GRU((768,512)|drop_out(0.3)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code (w/ lib) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sanity check when init to see if folders and custom methods are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel: RBF that can handle non-linear pattern\n",
    "C: Low to avoid overfit\n",
    "eps:  high values lead to a simpler model but potentially less precise predictions\n",
    "      low values require tighter predictions, which can make the model more complex\n",
    "\n",
    "WHAT I HAVE:\n",
    "\n",
    "CLIPS_REDUCTION values:\n",
    "  MEAN: 0 (applied in action recognition)\n",
    "  GRU: lstm (work in progress)\n",
    "\n",
    "EMBEDDING_REDUCTION values:\n",
    "  MEAN_TEMPORAL: 1      [keep spatial information]\n",
    "  MEAN_SPATIAL: (2, 3)  [keep temporal information]\n",
    "  MEAN_TEMPORAL_SPATIAL: (1, 2, 3) [applied in action recognition]\n",
    "  GRU: GRU (work in progress)\n",
    "\n",
    "MODEL_TYPE values:\n",
    "  VIDEOMAE_v2_S: smaller model\n",
    "  VIDEOMAE_v2_B: base model\n",
    "  VIDEOMAE_v2_G_pt_1200e: giant model w/h intermediate fine-tuning\n",
    "  VIDEOMAE_v2_G_pt_1200e_K710_it_HMDB51_ft: giant model fine-tuned\n",
    "\n",
    "SAMPLE_FRAME_STRATEGY values:\n",
    "  UNIFORM: uniform\n",
    "  SLIDING_WINDOW: sliding_window\n",
    "  CENTRAL_SAMPLING: central_sampling\n",
    "  RANDOM_SAMPLING: random_sampling\n",
    "\n",
    "HEAD\n",
    "  SVR\n",
    "____________________________________________________________________________\n",
    "\n",
    "\n",
    "TESTING SETTINGS GRID_SEARCH\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_B\n",
    "embedding_reduction = EMBEDDING_REDUCTION.MEAN_TEMPORAL_SPATIAL\n",
    "clips_reduction = CLIPS_REDUCTION.MEAN\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.UNIFORM\n",
    "\n",
    "path_labels = os.path.join('partA','starting_point','subsamples_100_400.csv') # 110 samples per class, 400 samples in total\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "k_cross validation = 5 (Stratified K-Fold cross-validator-> The folds are made by preserving the percentage of samples for each class.)\n",
    "\n",
    "grid_search = {\n",
    "  'kernel': ['rbf'],\n",
    "  'C': [0.1, 1, 10],\n",
    "  'epsilon': [0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "Form table we have:\n",
    "  best_estimator ={\n",
    "    kernel:'rbf',\n",
    "    'C': [0.1, 1, 10]\n",
    "    'epsilon':[10, 100]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow-x: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSPI from (Action Units)\n",
    "Pain expression is widely characterized by the activation of a small set of facial muscles and coded by a set of\n",
    "corresponding actions units (AUs): \n",
    "| Action units  | Meaning              | Range   |\n",
    "|----------------|----------------------|---------|\n",
    "| AU 4          | brow lowering        | 0 to 5  |\n",
    "| AU 6 and AU 7 | orbital tightening   | 0 to 5  |\n",
    "| AU 9 and AU 10| levator labii raise  | 0 to 5  |\n",
    "| AU 43         | eye closure          | 0 or 1  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 participants who did not react visibly to the applied pain stimuli:\n",
    "- ID:27, 082315_w_60 \n",
    "- ID:28, 082414_m_64\n",
    "- ID:32, 082909_m_47\n",
    "- ID:33, 083009_w_42\n",
    "- ID:34, 083013_w_47\n",
    "- ID:35, 083109_m_60\n",
    "- ID:36, 083114_w_55\n",
    "- ID:39, 091914_m_46\n",
    "- ID:40, 092009_m_54\n",
    "- ID:41, 092014_m_56\n",
    "- ID:42, 092509_w_51\n",
    "- ID:44, 092714_m_64\n",
    "- ID:51, 100514_w_51\n",
    "- ID:53, 100914_m_39\n",
    "- ID:55, 101114_w_37\n",
    "- ID:56, 101209_w_61\n",
    "- ID:61, 101809_m_59\n",
    "- ID:64, 101916_m_40\n",
    "- ID:74, 111313_m_64\n",
    "- ID:87, 120614_w_61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/villi/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/villi/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:459: UserWarning: Overwriting vit_small_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:474: UserWarning: Overwriting vit_base_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:489: UserWarning: Overwriting vit_large_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:519: UserWarning: Overwriting vit_giant_patch14_224 in registry with VideoMAEv2.models.modeling_finetune.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_224(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_all_features[\"list_subject_id\"] shape torch.Size([69600])\n",
      "Elasped time to get all features:  0.049924373626708984\n",
      "list_frames torch.Size([800, 16])\n",
      "list_sample_id torch.Size([800])\n",
      "list_video_path (800,)\n",
      "list_feature torch.Size([800, 8, 1, 1, 768])\n",
      "list_idx_list_frames (800,)\n",
      "list_y_gt torch.Size([800])\n",
      "TSNE_X.shape: torch.Size([800, 8, 1, 1, 768])\n",
      "Using CPU\n",
      "Start t-SNE computation...\n",
      "X_tsne shape: (800, 2)\n",
      "Plot saved to tsne_Results/test_1734624702/_.png\n",
      "axis_dict {'min_x': -45.911114586688164, 'min_y': -55.54092918217133, 'max_x': 46.468452529475016, 'max_y': 48.627928252559585}\n",
      "Plot saved to tsne_Results/test_1734624702/tsne_plot_16_clip/sliding_16_tot-subjects_1__clips_[0, 1, 2, 3, 4, 5, 6, 7]__classes_[4, 3, 2, 1, 0]_clip.png\n",
      "Elapsed time to get all plots: 85.62082552909851 s\n",
      "Generating video...\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-002.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-003.mp4\n",
      "Processed 10/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-010.mp4\n",
      "Processed 20/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-017.mp4\n",
      "Processed 30/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-020.mp4\n",
      "Processed 40/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-022.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-031.mp4\n",
      "Processed 50/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-036.mp4\n",
      "Processed 60/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-039.mp4\n",
      "Processed 70/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-041.mp4\n",
      "Processed 80/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-042.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-043.mp4\n",
      "Processed 90/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-049.mp4\n",
      "Processed 100/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-061.mp4\n",
      "Processed 110/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-062.mp4\n",
      "Processed 120/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-063.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-068.mp4\n",
      "Processed 130/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-069.mp4\n",
      "Processed 140/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-074.mp4\n",
      "Processed 150/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA4-076.mp4\n",
      "Processed 160/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-006.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-008.mp4\n",
      "Processed 170/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-009.mp4\n",
      "Processed 180/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-011.mp4\n",
      "Processed 190/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-015.mp4\n",
      "Processed 200/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-021.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-023.mp4\n",
      "Processed 210/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-024.mp4\n",
      "Processed 220/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-027.mp4\n",
      "Processed 230/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-029.mp4\n",
      "Processed 240/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-033.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-045.mp4\n",
      "Processed 250/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-046.mp4\n",
      "Processed 260/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-052.mp4\n",
      "Processed 270/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-055.mp4\n",
      "Processed 280/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-067.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-070.mp4\n",
      "Processed 290/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-072.mp4\n",
      "Processed 300/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-077.mp4\n",
      "Processed 310/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA3-079.mp4\n",
      "Processed 320/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-004.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-005.mp4\n",
      "Processed 330/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-013.mp4\n",
      "Processed 340/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-014.mp4\n",
      "Processed 350/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-016.mp4\n",
      "Processed 360/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-019.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-032.mp4\n",
      "Processed 370/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-034.mp4\n",
      "Processed 380/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-035.mp4\n",
      "Processed 390/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-038.mp4\n",
      "Processed 400/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-040.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-044.mp4\n",
      "Processed 410/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-050.mp4\n",
      "Processed 420/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-051.mp4\n",
      "Processed 430/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-056.mp4\n",
      "Processed 440/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-057.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-059.mp4\n",
      "Processed 450/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-064.mp4\n",
      "Processed 460/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-071.mp4\n",
      "Processed 470/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA2-080.mp4\n",
      "Processed 480/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-001.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-007.mp4\n",
      "Processed 490/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-012.mp4\n",
      "Processed 500/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-018.mp4\n",
      "Processed 510/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-025.mp4\n",
      "Processed 520/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-026.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-028.mp4\n",
      "Processed 530/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-030.mp4\n",
      "Processed 540/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-037.mp4\n",
      "Processed 550/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-047.mp4\n",
      "Processed 560/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-048.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-053.mp4\n",
      "Processed 570/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-054.mp4\n",
      "Processed 580/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-058.mp4\n",
      "Processed 590/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-060.mp4\n",
      "Processed 600/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-065.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-066.mp4\n",
      "Processed 610/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-073.mp4\n",
      "Processed 620/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-075.mp4\n",
      "Processed 630/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-PA1-078.mp4\n",
      "Processed 640/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-081.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-082.mp4\n",
      "Processed 650/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-083.mp4\n",
      "Processed 660/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-084.mp4\n",
      "Processed 670/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-085.mp4\n",
      "Processed 680/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-086.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-087.mp4\n",
      "Processed 690/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-088.mp4\n",
      "Processed 700/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-089.mp4\n",
      "Processed 710/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-090.mp4\n",
      "Processed 720/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-091.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-092.mp4\n",
      "Processed 730/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-093.mp4\n",
      "Processed 740/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-094.mp4\n",
      "Processed 750/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-095.mp4\n",
      "Processed 760/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-096.mp4\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-097.mp4\n",
      "Processed 770/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-098.mp4\n",
      "Processed 780/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-099.mp4\n",
      "Processed 790/800 videos\n",
      "video_path: partA/video/video/071309_w_21/071309_w_21-BL1-100.mp4\n",
      "Processed 800/800 videos\n",
      "Generated video saved to folder tsne_Results/test_1734624702/video\n",
      "Elapsed time to generate video: 153.08246564865112 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom.tools as tools\n",
    "import custom.scripts as scripts\n",
    "import os\n",
    "import time\n",
    "# OK finish the video clip, start to create plot list_same_clip_positions many people\n",
    "# try to combine plot and video in one\n",
    "folder_tsne_results = os.path.join('tsne_Results',f'test_{str(int(time.time()))}')\n",
    "folder_path_features = os.path.join('partA','video','features','samples_16')\n",
    "\n",
    "if not os.path.exists(folder_tsne_results):\n",
    "    os.makedirs(folder_tsne_results)\n",
    "\n",
    "# stoic subjects = [27,28,32,33,34,35,36,39,40,41,42,44,51,53,55,56,61,64,74,87]\n",
    "subject_id_list = [1]\n",
    "clip_list = [0,1,2,3,4,5,6,7]\n",
    "class_list = [4,3,2,1,0]\n",
    "sample_id_list = None\n",
    "sliding_windows =  16\n",
    "legend_label = 'clip' # can be clip, subject and class    \n",
    "scripts.plot_and_generate_video(folder_path_features=folder_path_features,\n",
    "                                folder_path_tsne_results=folder_tsne_results,\n",
    "                                subject_id_list=subject_id_list,\n",
    "                                clip_list=clip_list,\n",
    "                                legend_label=legend_label,\n",
    "                                class_list=class_list,\n",
    "                                sliding_windows=sliding_windows,\n",
    "                                # plot_only_sample_id_list=sample_id_list,\n",
    "                                plot_third_dim_time=False,\n",
    "                                create_video=True,\n",
    "                                apply_pca_before_tsne=False,\n",
    "                                tsne_n_component=2,\n",
    "                                sort_elements=True,\n",
    "                                cmap='copper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "frame_gpu = cv2.cuda_GpuMat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.dataset import customDataset\n",
    "from custom.backbone import backbone\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "path_labels = os.path.join('partA','starting_point','samples.csv')\n",
    "\n",
    "def _extract_features(dataset,path_csv_dataset,batch_size_feat_extraction,backbone):\n",
    "  \"\"\"\n",
    "  Extract features from the dataset specified by the CSV file path.\n",
    "\n",
    "  Args:\n",
    "    path_csv_dataset (str): Path to the CSV file containing dataset information.\n",
    "    batch_size (int, optional): Number of samples per batch to load. Default is 2.\n",
    "\n",
    "  Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "      - 'features' (torch.Tensor): shape [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim].\n",
    "      - 'list_labels' (torch.Tensor): shape [n_video * n_clips].\n",
    "      - 'list_subject_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_sample_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_path' (np.ndarray): shape (n_video * n_clips,).\n",
    "      - 'list_frames' (torch.Tensor): shape [n_video * n_clips, n_frames].\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  print(f\"extracting features using.... {device}\")\n",
    "  list_features = []\n",
    "  list_labels = []\n",
    "  list_subject_id = []\n",
    "  list_sample_id = []\n",
    "  list_path = []\n",
    "  list_frames = []\n",
    "  count = 0\n",
    "  dataset.set_path_labels(path_csv_dataset)\n",
    "  dataloader = DataLoader(dataset, \n",
    "                          batch_size=batch_size_feat_extraction,\n",
    "                          # num_workers=1,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=dataset._custom_collate_fn)\n",
    "  # move the model to the device\n",
    "  backbone.model.to(device)\n",
    "  backbone.model.eval()\n",
    "  with torch.no_grad():\n",
    "    # start_total_time = time.time()\n",
    "    start = time.time()\n",
    "    for data, labels, subject_id,sample_id, path, list_sampled_frames in dataloader:\n",
    "      #############################################################################################################\n",
    "      # data shape -> [nr_clips, clip_length=16, channels=3, H=224, W=224]\n",
    "      # \n",
    "      # nr_clips  = floor((total_frames-clip_length=16)/stride_window) + 1\n",
    "      #           BIOVID -> floor((138-16)/4)) + 1 = 31\n",
    "      # \n",
    "      # self.backbone.model ->   85 MB (small_model), \n",
    "      #                         400 MB (base_model), \n",
    "      #                           4 GB (giant_model)\n",
    "      # \n",
    "      # video_feat_size [nr_video,8,768] => 8700 * 8 * 768 * 4 = 204 MB\n",
    "      #############################################################################################################\n",
    "      # print(f'Elapsed time for {batch_size} samples: {time.time() - start}')\n",
    "      print(f'data shape {data.shape}')\n",
    "      data = data.to(device)\n",
    "      with torch.no_grad():\n",
    "    # Extract features from clips -> return [B, clips/tubelets, W/patch_w, H/patch_h, emb_dim] \n",
    "        feature = backbone.forward_features(x=data)\n",
    "      # feature -> [2, 8, 1, 1, 384]\n",
    "      list_frames.append(list_sampled_frames)\n",
    "      list_features.append(feature.detach().cpu())\n",
    "      list_labels.append(labels)\n",
    "      list_sample_id.append(sample_id)\n",
    "      list_subject_id.append(subject_id)\n",
    "      list_path.append(path)\n",
    "      count += 1\n",
    "      # if count % 10 == 0:\n",
    "      print(f'Batch {count}/{len(dataloader)}')\n",
    "      print(f' Time {int((time.time() - start)/60)} m : {int((time.time() - start)%60)} s')\n",
    "      print(f' GPU:\\n  Free : {torch.cuda.mem_get_info()[0]/1024/1024/1024:.2f} GB \\n  total: {torch.cuda.mem_get_info()[1]/1024/1024/1024:.2f} GB')\n",
    "      del data, feature\n",
    "      torch.cuda.empty_cache()\n",
    "      # start = time.time()\n",
    "  # print(f'Elapsed time for total feature extraction: {time.time() - start_total_time}')\n",
    "  # print('Feature extraceton done')\n",
    "  backbone.model.to('cpu')\n",
    "  # print('backbone moved to cpu')\n",
    "  # print(f'torch.cat features {torch.cat(list_features,dim=0).shape}')\n",
    "  dict_data = {\n",
    "    'features': torch.cat(list_features,dim=0),  # [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim] 630GB\n",
    "    'list_labels': torch.cat(list_labels,dim=0),  # [n_video * n_clips] 8700 * 10 * 4 = 340 KB\n",
    "    'list_subject_id': torch.cat(list_subject_id).squeeze(),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_sample_id': torch.cat(list_sample_id),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_path': np.concatenate(list_path),  # (n_video * n_clips,) 8700 * 10 * 4 = 340 KB\n",
    "    'list_frames': torch.cat(list_frames,dim=0)  # [n_video * n_clips, n_frames] 8700 * 10 * 4 = 340 KB\n",
    "  }\n",
    "\n",
    "  return dict_data \n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "custom_ds = customDataset(path_dataset=path_dataset,\n",
    "                          path_labels=path_labels,\n",
    "                          sample_frame_strategy=sample_frame_strategy,\n",
    "                          stride_window=4,\n",
    "                          preprocess=preprocess,\n",
    "                          clip_length=16)\n",
    "backbone_model = backbone(model_type=model_type)\n",
    "\n",
    "dict_data = _extract_features(dataset=custom_ds,\n",
    "                              path_csv_dataset=path_labels,\n",
    "                              batch_size_feat_extraction=1,\n",
    "                              backbone=backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data: Categories and corresponding values (including zeros)\n",
    "categories = [0, 1, 2, 3, 4]\n",
    "values = [5, 0, 3, 0, 8]  # Include zero values\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(categories, values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Bar Chart Including Zero-Value Bars')\n",
    "\n",
    "# Ensure y-axis starts at 0 for better visibility\n",
    "# plt.ylim(bottom=0)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/villi/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/villi/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:459: UserWarning: Overwriting vit_small_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:474: UserWarning: Overwriting vit_base_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:489: UserWarning: Overwriting vit_large_patch16_224 in registry with VideoMAEv2.models.modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/villi/Desktop/PainAssessmentVideo/VideoMAEv2/models/modeling_finetune.py:519: UserWarning: Overwriting vit_giant_patch14_224 in registry with VideoMAEv2.models.modeling_finetune.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_224(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set path_labels: partA/starting_point/samples.csv\n",
      "Start training phase the model at history_run/VIDEOMAE_v2_S_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1734711243\n",
      " list_splits_idxs [array([   0,    1,    2, ..., 8197, 8198, 8199]), array([ 100,  101,  102, ..., 8297, 8298, 8299]), array([ 200,  201,  202, ..., 8397, 8398, 8399]), array([ 300,  301,  302, ..., 8697, 8698, 8699]), array([ 400,  401,  402, ..., 8597, 8598, 8599])]\n",
      "CSV saved to history_run/VIDEOMAE_v2_S_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1734711243/train_GRU/k0_cross_val/test.csv\n",
      "CSV saved to history_run/VIDEOMAE_v2_S_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1734711243/train_GRU/k0_cross_val/val.csv\n",
      "CSV saved to history_run/VIDEOMAE_v2_S_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1734711243/train_GRU/k0_cross_val/train.csv\n",
      "training using validation set\n",
      "Extracting features...\n",
      "csv_path:history_run/VIDEOMAE_v2_S_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1734711243/train_GRU/k0_cross_val/train.csv\n",
      "Loading features from SSD...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import os\n",
    "from custom.model import Model_Advanced\n",
    "from transformers import AutoImageProcessor\n",
    "from custom.head import HeadSVR, HeadGRU\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "# path_dict ={\n",
    "#   'all' : os.path.join('partA','starting_point','samples.csv'),\n",
    "  # 'train' : os.path.join('partA','starting_point','train_21.csv'),\n",
    "  # 'val' : os.path.join('partA','starting_point','val_26.csv'),\n",
    "  # 'test' : os.path.join('partA','starting_point','test_5.csv')\n",
    "# }\n",
    "path_dataset = os.path.join('partA','video','video')  \n",
    "path_cvs_dataset = os.path.join('partA','starting_point','samples.csv')\n",
    "head = HEAD.GRU\n",
    "# if head == 'GRU':\n",
    "params = {\n",
    "  'hidden_size': 1024,\n",
    "  'num_layers': 1,\n",
    "  'dropout': 0.0,\n",
    "  'input_size': 768 * 8 # can be 384  (small), 768  (base), 1408  (large) [temporal_dim considered as input sequence for GRU]\n",
    "                    # can be 384*8(small), 768*8(base), 1408*8(large) [temporal_dim considered feature in GRU] \n",
    "}\n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "stride_window_in_video = 4\n",
    "model_advanced = scripts.run_train_test(model_type=model_type, \n",
    "                      pooling_embedding_reduction=pooling_embedding_reduction, \n",
    "                      pooling_clips_reduction=pooling_clips_reduction, \n",
    "                      sample_frame_strategy=sample_frame_strategy, \n",
    "                      path_csv_dataset=path_cvs_dataset, \n",
    "                      path_video_dataset=path_dataset,\n",
    "                      head=head,\n",
    "                      stride_window_in_video=stride_window_in_video, \n",
    "                      head_params=params,\n",
    "                      preprocess=preprocess,\n",
    "                      k_fold = 5,\n",
    "                      epochs = 5,\n",
    "                      train_size=0.8,\n",
    "                      test_size=0.1,\n",
    "                      val_size=0.1,\n",
    "                      batch_size_training=1024,\n",
    "                      batch_size_feat_extraction=8,  \n",
    "                      criterion = nn.L1Loss(),\n",
    "                      optimizer_fn = optim.SGD,\n",
    "                      lr = 0.0001,\n",
    "                      random_state_split_dataset=42,\n",
    "                      only_train=False,\n",
    "                      is_save_features_extracted=False, \n",
    "                      is_validation=True,\n",
    "                      is_plot_dataset_distribution=True,\n",
    "                      is_plot_loss=True,\n",
    "                      is_plot_tsne_backbone_feats=True,\n",
    "                      is_plot_tsne_head_pred=True,\n",
    "                      is_plot_tsne_gru_feats=True,\n",
    "                      is_create_video_prediction=True,\n",
    "                      is_create_video_prediction_per_video=True,\n",
    "                      is_round_output_loss=False,\n",
    "                      is_shuffle_training_batch=True,\n",
    "                      is_shuffle_video_chunks=False,\n",
    "                      is_download_if_unavailable=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_advanced.dataset.stride_window = 40\n",
    "scripts.predict_per_video(\n",
    "    path_csv='partA/starting_point/samples.csv',\n",
    "    sample_ids=[10,35],\n",
    "    model_advanced=model_advanced,\n",
    "    root_folder_path='history_run/VIDEOMAE_v2_B_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1733007378/train_GRU/video',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_working_directory():\n",
    "  target_dir = 'PainAssessmentVideo'\n",
    "  current_dir = os.getcwd()\n",
    "  if os.path.split(current_dir)[-1] != target_dir:\n",
    "    while os.path.split(current_dir)[-1] != target_dir:\n",
    "      current_dir = os.path.dirname(current_dir)\n",
    "      if current_dir == os.path.dirname(current_dir):  # reached the root directory\n",
    "        raise FileNotFoundError(f\"{target_dir} not found in the directory tree. Please set PainAssessmentVideo as current working directory.\")\n",
    "    os.chdir(current_dir)\n",
    "  print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def create_folder(path):\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(f'Folder created: {path}')\n",
    "  else:\n",
    "    print(f'Folder already exists: {path}')                                                                                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoMaev2_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
