{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoMAEv2 model availables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train\n",
    "\n",
    "| Model | Config | Dataset | \n",
    "| :---: | :----  | :-----: | \n",
    "| ViT-giant | vit_g_hybrid_pt_1200e | UnlabeledHybrid | \n",
    "\n",
    "### Fine-tune\n",
    "| Model | Config | Dataset | Pre-train | Post-pre-train |\n",
    "| :---: | :----  | :-----: | :-------: | :------------: |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_ft | K710 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k400_ft | K400 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k400_ft | K400 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k600_ft | K600 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_ssv2_ft | SSv2 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_ucf101_ft | UCF101 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_hmdb51_ft | HMDB51 | UnlabeledHybrid | K710 |\n",
    "\n",
    "### Distillation from giant\n",
    "|  Model  | Dataset | Teacher Model  |\n",
    "| :-----: | :-----: | :-----------: |\n",
    "| ViT-small | K710 | vit_g_hybrid_pt_1200e_k710_ft |\n",
    "| ViT-base | K710 | vit_g_hybrid_pt_1200e_k710_ft | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model details\n",
    "\n",
    "|  model  | frame channels | frame sampling | frame size (H,W) | tubelet size | patch size | emb dim | output tensor | mem(GB) |\n",
    "| :-----: | :-----: | :-----------: | :-----: | :-----: | :-----------: | :-----: | :-----: |:----|\n",
    "| giant | 3 | 16 | (224,224) | 2 | (14,14) | 1408 | [8,16,16,1408] | 4.0 |\n",
    "| base | 3 | 16 | (224,224) | 2 | (16,16) | 768 | [8,14,14,768] | 0.4|\n",
    "| small | 3 | 16 | (224,224) | 2 | (16,16) | 384 | [8,14,14,1408] | 0.09|\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE 1\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8,(8*1*1*768)] =>\n",
    "    => 2 GRU((6144,512)|dropout(0.5)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "\n",
    "CASE 2\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8*8,(1*1*768)] =>\n",
    "    => 2 GRU((768,512)|drop_out(0.3)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code (w/ lib) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sanity check when init to see if folders and custom methods are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel: RBF that can handle non-linear pattern\n",
    "C: Low to avoid overfit\n",
    "eps:  high values lead to a simpler model but potentially less precise predictions\n",
    "      low values require tighter predictions, which can make the model more complex\n",
    "\n",
    "WHAT I HAVE:\n",
    "\n",
    "CLIPS_REDUCTION values:\n",
    "  MEAN: 0 (applied in action recognition)\n",
    "  GRU: lstm (work in progress)\n",
    "\n",
    "EMBEDDING_REDUCTION values:\n",
    "  MEAN_TEMPORAL: 1      [keep spatial information]\n",
    "  MEAN_SPATIAL: (2, 3)  [keep temporal information]\n",
    "  MEAN_TEMPORAL_SPATIAL: (1, 2, 3) [applied in action recognition]\n",
    "  GRU: GRU (work in progress)\n",
    "\n",
    "MODEL_TYPE values:\n",
    "  VIDEOMAE_v2_S: smaller model\n",
    "  VIDEOMAE_v2_B: base model\n",
    "  VIDEOMAE_v2_G_pt_1200e: giant model w/h intermediate fine-tuning\n",
    "  VIDEOMAE_v2_G_pt_1200e_K710_it_HMDB51_ft: giant model fine-tuned\n",
    "\n",
    "SAMPLE_FRAME_STRATEGY values:\n",
    "  UNIFORM: uniform\n",
    "  SLIDING_WINDOW: sliding_window\n",
    "  CENTRAL_SAMPLING: central_sampling\n",
    "  RANDOM_SAMPLING: random_sampling\n",
    "\n",
    "HEAD\n",
    "  SVR\n",
    "____________________________________________________________________________\n",
    "\n",
    "\n",
    "TESTING SETTINGS GRID_SEARCH\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_B\n",
    "embedding_reduction = EMBEDDING_REDUCTION.MEAN_TEMPORAL_SPATIAL\n",
    "clips_reduction = CLIPS_REDUCTION.MEAN\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.UNIFORM\n",
    "\n",
    "path_labels = os.path.join('partA','starting_point','subsamples_100_400.csv') # 110 samples per class, 400 samples in total\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "k_cross validation = 5 (Stratified K-Fold cross-validator-> The folds are made by preserving the percentage of samples for each class.)\n",
    "\n",
    "grid_search = {\n",
    "  'kernel': ['rbf'],\n",
    "  'C': [0.1, 1, 10],\n",
    "  'epsilon': [0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "Form table we have:\n",
    "  best_estimator ={\n",
    "    kernel:'rbf',\n",
    "    'C': [0.1, 1, 10]\n",
    "    'epsilon':[10, 100]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow-x: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vector for each clip -> [8,1,1,768] \n",
    "subject_id_list: [1, 3, 5, 7, 21, 25, 27, 28] # tot:8\n",
    "clips: \n",
    "  sliding 16: [0, 3, 4, 5]    # {0:[0,15]  3:[48,63] 4:[64,79] 5:[80,95]}\n",
    "  sliding 12: [0, 4, 5, 6, 7] # {0:[0,11]  4:[48,59] 5:[60,71] 6:[72,83] 7:[84,95]}\n",
    "classes: [4]\n",
    "Plot per clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8604272880038479\n",
      "3.337538441992365\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Using range\n",
    "print(timeit.timeit('for i in range(1000): pass', number=100000))\n",
    "\n",
    "# Using np.arange\n",
    "import numpy as np\n",
    "print(timeit.timeit('for i in np.arange(1000): pass', globals={\"np\": np}, number=100000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Elasped time to get all features:  0.06517720222473145\n",
      "list_frames torch.Size([8700, 16])\n",
      "list_sample_id torch.Size([8700])\n",
      "list_video_path (8700,)\n",
      "list_feature torch.Size([8700, 8, 1, 1, 768])\n",
      "list_idx_list_frames (8700,)\n",
      "list_y_gt torch.Size([8700])\n",
      "TSNE_X.shape: torch.Size([8700, 8, 1, 1, 768])\n",
      "Using CPU\n",
      "PCA using 100 components...\n",
      "Start t-SNE computation...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom.tools as tools\n",
    "import custom.scripts as scripts\n",
    "import os\n",
    "import time\n",
    "# OK finish the video clip, start to create plot list_same_clip_positions many people\n",
    "# try to combine plot and video in one\n",
    "folder_tsne_results = os.path.join('tsne_Results',f'test_{str(int(time.time()))}')\n",
    "folder_path_features = os.path.join('partA','video','features','samples_16')\n",
    "\n",
    "if not os.path.exists(folder_tsne_results):\n",
    "    os.makedirs(folder_tsne_results)\n",
    "\n",
    "subject_id_list = list(range(1,88))\n",
    "clip_list = [0]\n",
    "class_list = [0,1,2,3,4]\n",
    "sample_id_list = None\n",
    "sliding_windows =  16\n",
    "legend_label = 'clip' # can be clip, subject and class    \n",
    "scripts.plot_and_generate_video(folder_path_features=folder_path_features,\n",
    "                                folder_path_tsne_results=folder_tsne_results,\n",
    "                                subject_id_list=subject_id_list,\n",
    "                                clip_list=clip_list,\n",
    "                                legend_label=legend_label,\n",
    "                                class_list=class_list,\n",
    "                                sliding_windows=sliding_windows,\n",
    "                                # plot_only_sample_id_list=sample_id_list,\n",
    "                                plot_third_dim_time=False,\n",
    "                                create_video=False,\n",
    "                                apply_pca_before_tsne=True,\n",
    "                                tsne_n_component=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.dataset import customDataset\n",
    "from custom.backbone import backbone\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "path_labels = os.path.join('partA','starting_point','samples.csv')\n",
    "\n",
    "def _extract_features(dataset,path_csv_dataset,batch_size_feat_extraction,backbone):\n",
    "  \"\"\"\n",
    "  Extract features from the dataset specified by the CSV file path.\n",
    "\n",
    "  Args:\n",
    "    path_csv_dataset (str): Path to the CSV file containing dataset information.\n",
    "    batch_size (int, optional): Number of samples per batch to load. Default is 2.\n",
    "\n",
    "  Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "      - 'features' (torch.Tensor): shape [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim].\n",
    "      - 'list_labels' (torch.Tensor): shape [n_video * n_clips].\n",
    "      - 'list_subject_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_sample_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_path' (np.ndarray): shape (n_video * n_clips,).\n",
    "      - 'list_frames' (torch.Tensor): shape [n_video * n_clips, n_frames].\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  print(f\"extracting features using.... {device}\")\n",
    "  list_features = []\n",
    "  list_labels = []\n",
    "  list_subject_id = []\n",
    "  list_sample_id = []\n",
    "  list_path = []\n",
    "  list_frames = []\n",
    "  count = 0\n",
    "  dataset.set_path_labels(path_csv_dataset)\n",
    "  dataloader = DataLoader(dataset, \n",
    "                          batch_size=batch_size_feat_extraction,\n",
    "                          # num_workers=1,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=dataset._custom_collate_fn)\n",
    "  # move the model to the device\n",
    "  backbone.model.to(device)\n",
    "  backbone.model.eval()\n",
    "  with torch.no_grad():\n",
    "    # start_total_time = time.time()\n",
    "    start = time.time()\n",
    "    for data, labels, subject_id,sample_id, path, list_sampled_frames in dataloader:\n",
    "      #############################################################################################################\n",
    "      # data shape -> [nr_clips, clip_length=16, channels=3, H=224, W=224]\n",
    "      # \n",
    "      # nr_clips  = floor((total_frames-clip_length=16)/stride_window) + 1\n",
    "      #           BIOVID -> floor((138-16)/4)) + 1 = 31\n",
    "      # \n",
    "      # self.backbone.model ->   85 MB (small_model), \n",
    "      #                         400 MB (base_model), \n",
    "      #                           4 GB (giant_model)\n",
    "      # \n",
    "      # video_feat_size [nr_video,8,768] => 8700 * 8 * 768 * 4 = 204 MB\n",
    "      #############################################################################################################\n",
    "      # print(f'Elapsed time for {batch_size} samples: {time.time() - start}')\n",
    "      print(f'data shape {data.shape}')\n",
    "      data = data.to(device)\n",
    "      with torch.no_grad():\n",
    "    # Extract features from clips -> return [B, clips/tubelets, W/patch_w, H/patch_h, emb_dim] \n",
    "        feature = backbone.forward_features(x=data)\n",
    "      # feature -> [2, 8, 1, 1, 384]\n",
    "      list_frames.append(list_sampled_frames)\n",
    "      list_features.append(feature.detach().cpu())\n",
    "      list_labels.append(labels)\n",
    "      list_sample_id.append(sample_id)\n",
    "      list_subject_id.append(subject_id)\n",
    "      list_path.append(path)\n",
    "      count += 1\n",
    "      # if count % 10 == 0:\n",
    "      print(f'Batch {count}/{len(dataloader)}')\n",
    "      print(f' Time {int((time.time() - start)/60)} m : {int((time.time() - start)%60)} s')\n",
    "      print(f' GPU:\\n  Free : {torch.cuda.mem_get_info()[0]/1024/1024/1024:.2f} GB \\n  total: {torch.cuda.mem_get_info()[1]/1024/1024/1024:.2f} GB')\n",
    "      del data, feature\n",
    "      torch.cuda.empty_cache()\n",
    "      # start = time.time()\n",
    "  # print(f'Elapsed time for total feature extraction: {time.time() - start_total_time}')\n",
    "  # print('Feature extraceton done')\n",
    "  backbone.model.to('cpu')\n",
    "  # print('backbone moved to cpu')\n",
    "  # print(f'torch.cat features {torch.cat(list_features,dim=0).shape}')\n",
    "  dict_data = {\n",
    "    'features': torch.cat(list_features,dim=0),  # [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim] 630GB\n",
    "    'list_labels': torch.cat(list_labels,dim=0),  # [n_video * n_clips] 8700 * 10 * 4 = 340 KB\n",
    "    'list_subject_id': torch.cat(list_subject_id).squeeze(),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_sample_id': torch.cat(list_sample_id),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_path': np.concatenate(list_path),  # (n_video * n_clips,) 8700 * 10 * 4 = 340 KB\n",
    "    'list_frames': torch.cat(list_frames,dim=0)  # [n_video * n_clips, n_frames] 8700 * 10 * 4 = 340 KB\n",
    "  }\n",
    "\n",
    "  return dict_data \n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "custom_ds = customDataset(path_dataset=path_dataset,\n",
    "                          path_labels=path_labels,\n",
    "                          sample_frame_strategy=sample_frame_strategy,\n",
    "                          stride_window=4,\n",
    "                          preprocess=preprocess,\n",
    "                          clip_length=16)\n",
    "backbone_model = backbone(model_type=model_type)\n",
    "\n",
    "dict_data = _extract_features(dataset=custom_ds,\n",
    "                              path_csv_dataset=path_labels,\n",
    "                              batch_size_feat_extraction=1,\n",
    "                              backbone=backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import os\n",
    "from custom.model import Model_Advanced\n",
    "from transformers import AutoImageProcessor\n",
    "from custom.head import HeadSVR, HeadGRU\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "# path_dict ={\n",
    "#   'all' : os.path.join('partA','starting_point','samples.csv'),\n",
    "  # 'train' : os.path.join('partA','starting_point','train_21.csv'),\n",
    "  # 'val' : os.path.join('partA','starting_point','val_26.csv'),\n",
    "  # 'test' : os.path.join('partA','starting_point','test_5.csv')\n",
    "# }\n",
    "path_dataset = os.path.join('partA','video','video')  \n",
    "path_cvs_dataset = os.path.join('partA','starting_point','samples.csv')\n",
    "head = HEAD.GRU\n",
    "# if head == 'GRU':\n",
    "params = {\n",
    "  'hidden_size': 1024,\n",
    "  'num_layers': 1,\n",
    "  'dropout': 0.0,\n",
    "  'input_size': 768 * 8 # can be 384  (small), 768  (base), 1408  (large) [temporal_dim considered as input sequence for GRU]\n",
    "                    # can be 384*8(small), 768*8(base), 1408*8(large) [temporal_dim considered feature in GRU] \n",
    "}\n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "stride_window_in_video = 16\n",
    "model_advanced = scripts.run_train_test(model_type=model_type, \n",
    "                      pooling_embedding_reduction=pooling_embedding_reduction, \n",
    "                      pooling_clips_reduction=pooling_clips_reduction, \n",
    "                      sample_frame_strategy=sample_frame_strategy, \n",
    "                      path_csv_dataset=path_cvs_dataset, \n",
    "                      path_video_dataset=path_dataset,\n",
    "                      head=head,\n",
    "                      stride_window_in_video=stride_window_in_video, \n",
    "                      head_params=params,\n",
    "                      preprocess=preprocess,\n",
    "                      k_fold = 5,\n",
    "                      epochs = 300,\n",
    "                      train_size=0.8,\n",
    "                      test_size=0.1,\n",
    "                      val_size=0.1,\n",
    "                      batch_size_training=1024,\n",
    "                      batch_size_feat_extraction=8,  \n",
    "                      criterion = nn.L1Loss(),\n",
    "                      optimizer_fn = optim.SGD,\n",
    "                      lr = 0.0001,\n",
    "                      random_state_split_dataset=42,\n",
    "                      only_train=False,\n",
    "                      is_save_features_extracted=False, \n",
    "                      is_validation=True,\n",
    "                      is_plot_dataset_distribution=True,\n",
    "                      is_plot_loss=True,\n",
    "                      is_plot_tsne_backbone_feats=True,\n",
    "                      is_plot_tsne_head_pred=True,\n",
    "                      is_plot_tsne_gru_feats=True,\n",
    "                      is_create_video_prediction=True,\n",
    "                      is_create_video_prediction_per_video=True,\n",
    "                      is_round_output_loss=False,\n",
    "                      is_shuffle_training_batch=True,\n",
    "                      is_shuffle_video_chunks=False,\n",
    "                      is_download_if_unavailable=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_advanced.dataset.stride_window = 40\n",
    "scripts.predict_per_video(\n",
    "    path_csv='partA/starting_point/samples.csv',\n",
    "    sample_ids=[10,35],\n",
    "    model_advanced=model_advanced,\n",
    "    root_folder_path='history_run/VIDEOMAE_v2_B_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1733007378/train_GRU/video',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_working_directory():\n",
    "  target_dir = 'PainAssessmentVideo'\n",
    "  current_dir = os.getcwd()\n",
    "  if os.path.split(current_dir)[-1] != target_dir:\n",
    "    while os.path.split(current_dir)[-1] != target_dir:\n",
    "      current_dir = os.path.dirname(current_dir)\n",
    "      if current_dir == os.path.dirname(current_dir):  # reached the root directory\n",
    "        raise FileNotFoundError(f\"{target_dir} not found in the directory tree. Please set PainAssessmentVideo as current working directory.\")\n",
    "    os.chdir(current_dir)\n",
    "  print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def create_folder(path):\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(f'Folder created: {path}')\n",
    "  else:\n",
    "    print(f'Folder already exists: {path}')                                                                                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoMaev2_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
