{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoMAEv2 model availables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train\n",
    "\n",
    "| Model | Config | Dataset | \n",
    "| :---: | :----  | :-----: | \n",
    "| ViT-giant | vit_g_hybrid_pt_1200e | UnlabeledHybrid | \n",
    "\n",
    "### Fine-tune\n",
    "| Model | Config | Dataset | Pre-train | Post-pre-train |\n",
    "| :---: | :----  | :-----: | :-------: | :------------: |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_ft | K710 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k400_ft | K400 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k400_ft | K400 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_k600_ft | K600 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_ssv2_ft | SSv2 | UnlabeledHybrid | None |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_ucf101_ft | UCF101 | UnlabeledHybrid | K710 |\n",
    "| ViT-giant | vit_g_hybrid_pt_1200e_k710_it_hmdb51_ft | HMDB51 | UnlabeledHybrid | K710 |\n",
    "\n",
    "### Distillation from giant\n",
    "|  Model  | Dataset | Teacher Model  |\n",
    "| :-----: | :-----: | :-----------: |\n",
    "| ViT-small | K710 | vit_g_hybrid_pt_1200e_k710_ft |\n",
    "| ViT-base | K710 | vit_g_hybrid_pt_1200e_k710_ft | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model details\n",
    "\n",
    "|  model  | frame channels | frame sampling | frame size (H,W) | tubelet size | patch size | emb dim | output tensor | mem(GB) |\n",
    "| :-----: | :-----: | :-----------: | :-----: | :-----: | :-----------: | :-----: | :-----: |:----|\n",
    "| giant | 3 | 16 | (224,224) | 2 | (14,14) | 1408 | [8,16,16,1408] | 4.0 |\n",
    "| base | 3 | 16 | (224,224) | 2 | (16,16) | 768 | [8,14,14,768] | 0.4|\n",
    "| small | 3 | 16 | (224,224) | 2 | (16,16) | 384 | [8,14,14,1408] | 0.09|\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE 1\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8,(8*1*1*768)] =>\n",
    "    => 2 GRU((6144,512)|dropout(0.5)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "\n",
    "CASE 2\n",
    "flow -> [batch_video,nr_windows=8,(8,14,14,768)] => \n",
    "    => spatial mean reduction =>\n",
    "    =>[batch_video,nr_windows=8,(8,1,1,768)] => \n",
    "    => RESHAPE =>\n",
    "    => [batch_video,nr_windows=8*8,(1*1*768)] =>\n",
    "    => 2 GRU((768,512)|drop_out(0.3)|(512,512)) + linear proj (512,1) =>\n",
    "    => [batch_video,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code (w/ lib) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sanity check when init to see if folders and custom methods are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel: RBF that can handle non-linear pattern\n",
    "C: Low to avoid overfit\n",
    "eps:  high values lead to a simpler model but potentially less precise predictions\n",
    "      low values require tighter predictions, which can make the model more complex\n",
    "\n",
    "WHAT I HAVE:\n",
    "\n",
    "CLIPS_REDUCTION values:\n",
    "  MEAN: 0 (applied in action recognition)\n",
    "  GRU: lstm (work in progress)\n",
    "\n",
    "EMBEDDING_REDUCTION values:\n",
    "  MEAN_TEMPORAL: 1      [keep spatial information]\n",
    "  MEAN_SPATIAL: (2, 3)  [keep temporal information]\n",
    "  MEAN_TEMPORAL_SPATIAL: (1, 2, 3) [applied in action recognition]\n",
    "  GRU: GRU (work in progress)\n",
    "\n",
    "MODEL_TYPE values:\n",
    "  VIDEOMAE_v2_S: smaller model\n",
    "  VIDEOMAE_v2_B: base model\n",
    "  VIDEOMAE_v2_G_pt_1200e: giant model w/h intermediate fine-tuning\n",
    "  VIDEOMAE_v2_G_pt_1200e_K710_it_HMDB51_ft: giant model fine-tuned\n",
    "\n",
    "SAMPLE_FRAME_STRATEGY values:\n",
    "  UNIFORM: uniform\n",
    "  SLIDING_WINDOW: sliding_window\n",
    "  CENTRAL_SAMPLING: central_sampling\n",
    "  RANDOM_SAMPLING: random_sampling\n",
    "\n",
    "HEAD\n",
    "  SVR\n",
    "____________________________________________________________________________\n",
    "\n",
    "\n",
    "TESTING SETTINGS GRID_SEARCH\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_B\n",
    "embedding_reduction = EMBEDDING_REDUCTION.MEAN_TEMPORAL_SPATIAL\n",
    "clips_reduction = CLIPS_REDUCTION.MEAN\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.UNIFORM\n",
    "\n",
    "path_labels = os.path.join('partA','starting_point','subsamples_100_400.csv') # 110 samples per class, 400 samples in total\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "k_cross validation = 5 (Stratified K-Fold cross-validator-> The folds are made by preserving the percentage of samples for each class.)\n",
    "\n",
    "grid_search = {\n",
    "  'kernel': ['rbf'],\n",
    "  'C': [0.1, 1, 10],\n",
    "  'epsilon': [0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "Form table we have:\n",
    "  best_estimator ={\n",
    "    kernel:'rbf',\n",
    "    'C': [0.1, 1, 10]\n",
    "    'epsilon':[10, 100]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow-x: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vector for each clip -> [8,1,1,768] \n",
    "subject_id_list: [1, 3, 5, 7, 21, 25, 27, 28] # tot:8\n",
    "clips: \n",
    "  sliding 16: [0, 3, 4, 5]    # {0:[0,15]  3:[48,63] 4:[64,79] 5:[80,95]}\n",
    "  sliding 12: [0, 4, 5, 6, 7] # {0:[0,11]  4:[48,59] 5:[60,71] 6:[72,83] 7:[84,95]}\n",
    "classes: [4]\n",
    "Plot per clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "shape idx_subjects (69600,)\n",
      "Elasped time to get all features:  0.03819131851196289\n",
      "list_frames (80, 16)\n",
      "list_sample_id (80,)\n",
      "list_video_path (80,)\n",
      "list_feature (80, 8, 1, 1, 768)\n",
      "list_idx_list_frames (80,)\n",
      "list_y_gt (80,)\n",
      "TSNE_X.shape: torch.Size([80, 8, 1, 1, 768])\n",
      "Using CPU\n",
      "PCA using 80 components...\n",
      "Start t-SNE computation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m sliding_windows \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     20\u001b[0m legend_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# can be clip, subject and class    \u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_and_generate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_path_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfolder_path_tsne_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_tsne_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msubject_id_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject_id_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mclip_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlegend_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mclass_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msliding_windows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mplot_only_sample_id_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_id_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mplot_third_dim_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcreate_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mapply_pca_before_tsne\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtsne_n_component\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PainAssessmentVideo/custom/scripts.py:166\u001b[0m, in \u001b[0;36mplot_and_generate_video\u001b[0;34m(folder_path_features, folder_path_tsne_results, subject_id_list, clip_list, class_list, sliding_windows, legend_label, create_video, plot_only_sample_id_list, tsne_n_component, plot_third_dim_time, apply_pca_before_tsne)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_y_gt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_y_gt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m tsne_plot_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path_tsne_results,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsne_plot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msliding_windows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlegend_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m X_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_tsne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_feature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_tsne_results\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdummy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtsne_n_component\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtsne_n_component\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mapply_pca_before_tsne\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_pca_before_tsne\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# add 3th dimension to X_tsne\u001b[39;00m\n\u001b[1;32m    172\u001b[0m list_axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PainAssessmentVideo/custom/tools.py:615\u001b[0m, in \u001b[0;36mplot_tsne\u001b[0;34m(X, labels, tsne_n_component, apply_pca_before_tsne, legend_label, title, use_cuda, perplexity, saving_path, plot)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# print(f' X_cpu.shape: {X_cpu.shape}')\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart t-SNE computation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 615\u001b[0m X_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_cpu\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# OpenTSNE\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# get the folder of saving_path\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# print(f'path {os.path.split(saving_path)[:-1]}')\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saving_path:\n",
      "File \u001b[0;32m~/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/openTSNE/tsne.py:1262\u001b[0m, in \u001b[0;36mTSNE.fit\u001b[0;34m(self, X, affinities, initialization)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     embedding\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m   1253\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_exaggeration_iter,\n\u001b[1;32m   1254\u001b[0m         exaggeration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_exaggeration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         propagate_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;66;03m# Restore actual affinity probabilities and increase momentum to get\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;66;03m# final, optimized embedding\u001b[39;00m\n\u001b[0;32m-> 1262\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexaggeration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexaggeration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpropagate_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptimizationInterrupt \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1271\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization was interrupted with callback.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/openTSNE/tsne.py:679\u001b[0m, in \u001b[0;36mTSNEEmbedding.optimize\u001b[0;34m(self, n_iter, inplace, propagate_exception, **gradient_descent_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m _handle_nice_params(embedding, optim_params)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# Run gradient descent with the embedding optimizer so gains are\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# properly updated and kept\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     error, embedding \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffinities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptim_params\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptimizationInterrupt \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    684\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization was interrupted with callback.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/openTSNE/tsne.py:1790\u001b[0m, in \u001b[0;36mgradient_descent.__call__\u001b[0;34m(self, embedding, P, n_iter, objective_function, learning_rate, momentum, exaggeration, dof, min_gain, max_grad_norm, max_step_norm, theta, n_interpolation_points, min_num_intervals, ints_in_interval, reference_embedding, n_jobs, use_callbacks, callbacks, callbacks_every_iters, verbose)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# Evaluate error on 50 iterations for logging, or when callbacks\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m should_eval_error \u001b[38;5;241m=\u001b[39m should_call_callback \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m   1788\u001b[0m     (verbose \u001b[38;5;129;01mand\u001b[39;00m (iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1790\u001b[0m error, gradient \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbh_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbh_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfft_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfft_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshould_eval_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_eval_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;66;03m# Clip gradients to avoid points shooting off. This can be an issue\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;66;03m# when applying transform and points are initialized so that the new\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;66;03m# points overlap with the reference points, leading to large\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;66;03m# gradients\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/videoMaev2_project/lib/python3.10/site-packages/openTSNE/tsne.py:1470\u001b[0m, in \u001b[0;36mkl_divergence_bh\u001b[0;34m(embedding, P, dof, bh_params, reference_embedding, should_eval_error, n_jobs, **_)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# Compute negative gradient\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m tree \u001b[38;5;241m=\u001b[39m QuadTree(reference_embedding)\n\u001b[0;32m-> 1470\u001b[0m sum_Q \u001b[38;5;241m=\u001b[39m \u001b[43m_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_negative_gradient_bh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbh_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpairwise_normalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairwise_normalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tree\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# Compute positive gradient\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom.tools as tools\n",
    "import custom.scripts as scripts\n",
    "import os\n",
    "import time\n",
    "# OK finish the video clip, start to create plot list_same_clip_positions many people\n",
    "# try to combine plot and video in one\n",
    "folder_tsne_results = os.path.join('tsne_Results',f'test_{str(int(time.time()))}')\n",
    "folder_path_features = os.path.join('partA','video','features','samples_16')\n",
    "\n",
    "if not os.path.exists(folder_tsne_results):\n",
    "    os.makedirs(folder_tsne_results)\n",
    "\n",
    "subject_id_list = [1]\n",
    "clip_list = [0,1,8,9]\n",
    "class_list = [0,4]\n",
    "sample_id_list = None\n",
    "sliding_windows =  16\n",
    "legend_label = 'subject' # can be clip, subject and class    \n",
    "scripts.plot_and_generate_video(folder_path_features=folder_path_features,\n",
    "                                folder_path_tsne_results=folder_tsne_results,\n",
    "                                subject_id_list=subject_id_list,\n",
    "                                clip_list=clip_list,\n",
    "                                legend_label=legend_label,\n",
    "                                class_list=class_list,\n",
    "                                sliding_windows=sliding_windows,\n",
    "                                plot_only_sample_id_list=sample_id_list,\n",
    "                                plot_third_dim_time=False,\n",
    "                                create_video=False,\n",
    "                                apply_pca_before_tsne=True,\n",
    "                                tsne_n_component=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.dataset import customDataset\n",
    "from custom.backbone import backbone\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "\n",
    "path_dataset = os.path.join('partA','video','video')\n",
    "path_labels = os.path.join('partA','starting_point','samples.csv')\n",
    "\n",
    "def _extract_features(dataset,path_csv_dataset,batch_size_feat_extraction,backbone):\n",
    "  \"\"\"\n",
    "  Extract features from the dataset specified by the CSV file path.\n",
    "\n",
    "  Args:\n",
    "    path_csv_dataset (str): Path to the CSV file containing dataset information.\n",
    "    batch_size (int, optional): Number of samples per batch to load. Default is 2.\n",
    "\n",
    "  Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "      - 'features' (torch.Tensor): shape [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim].\n",
    "      - 'list_labels' (torch.Tensor): shape [n_video * n_clips].\n",
    "      - 'list_subject_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_sample_id' (torch.Tensor): shape (n_video * n_clips).\n",
    "      - 'list_path' (np.ndarray): shape (n_video * n_clips,).\n",
    "      - 'list_frames' (torch.Tensor): shape [n_video * n_clips, n_frames].\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  print(f\"extracting features using.... {device}\")\n",
    "  list_features = []\n",
    "  list_labels = []\n",
    "  list_subject_id = []\n",
    "  list_sample_id = []\n",
    "  list_path = []\n",
    "  list_frames = []\n",
    "  count = 0\n",
    "  dataset.set_path_labels(path_csv_dataset)\n",
    "  dataloader = DataLoader(dataset, \n",
    "                          batch_size=batch_size_feat_extraction,\n",
    "                          # num_workers=1,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=dataset._custom_collate_fn)\n",
    "  # move the model to the device\n",
    "  backbone.model.to(device)\n",
    "  backbone.model.eval()\n",
    "  with torch.no_grad():\n",
    "    # start_total_time = time.time()\n",
    "    start = time.time()\n",
    "    for data, labels, subject_id,sample_id, path, list_sampled_frames in dataloader:\n",
    "      #############################################################################################################\n",
    "      # data shape -> [nr_clips, clip_length=16, channels=3, H=224, W=224]\n",
    "      # \n",
    "      # nr_clips  = floor((total_frames-clip_length=16)/stride_window) + 1\n",
    "      #           BIOVID -> floor((138-16)/4)) + 1 = 31\n",
    "      # \n",
    "      # self.backbone.model ->   85 MB (small_model), \n",
    "      #                         400 MB (base_model), \n",
    "      #                           4 GB (giant_model)\n",
    "      # \n",
    "      # video_feat_size [nr_video,8,768] => 8700 * 8 * 768 * 4 = 204 MB\n",
    "      #############################################################################################################\n",
    "      # print(f'Elapsed time for {batch_size} samples: {time.time() - start}')\n",
    "      print(f'data shape {data.shape}')\n",
    "      data = data.to(device)\n",
    "      with torch.no_grad():\n",
    "    # Extract features from clips -> return [B, clips/tubelets, W/patch_w, H/patch_h, emb_dim] \n",
    "        feature = backbone.forward_features(x=data)\n",
    "      # feature -> [2, 8, 1, 1, 384]\n",
    "      list_frames.append(list_sampled_frames)\n",
    "      list_features.append(feature.detach().cpu())\n",
    "      list_labels.append(labels)\n",
    "      list_sample_id.append(sample_id)\n",
    "      list_subject_id.append(subject_id)\n",
    "      list_path.append(path)\n",
    "      count += 1\n",
    "      # if count % 10 == 0:\n",
    "      print(f'Batch {count}/{len(dataloader)}')\n",
    "      print(f' Time {int((time.time() - start)/60)} m : {int((time.time() - start)%60)} s')\n",
    "      print(f' GPU:\\n  Free : {torch.cuda.mem_get_info()[0]/1024/1024/1024:.2f} GB \\n  total: {torch.cuda.mem_get_info()[1]/1024/1024/1024:.2f} GB')\n",
    "      del data, feature\n",
    "      torch.cuda.empty_cache()\n",
    "      # start = time.time()\n",
    "  # print(f'Elapsed time for total feature extraction: {time.time() - start_total_time}')\n",
    "  # print('Feature extraceton done')\n",
    "  backbone.model.to('cpu')\n",
    "  # print('backbone moved to cpu')\n",
    "  # print(f'torch.cat features {torch.cat(list_features,dim=0).shape}')\n",
    "  dict_data = {\n",
    "    'features': torch.cat(list_features,dim=0),  # [n_video * n_clips, temporal_dim=8, patch_h, patch_w, emb_dim] 630GB\n",
    "    'list_labels': torch.cat(list_labels,dim=0),  # [n_video * n_clips] 8700 * 10 * 4 = 340 KB\n",
    "    'list_subject_id': torch.cat(list_subject_id).squeeze(),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_sample_id': torch.cat(list_sample_id),  # (n_video * n_clips) 8700 * 10 * 4 = 340 KB\n",
    "    'list_path': np.concatenate(list_path),  # (n_video * n_clips,) 8700 * 10 * 4 = 340 KB\n",
    "    'list_frames': torch.cat(list_frames,dim=0)  # [n_video * n_clips, n_frames] 8700 * 10 * 4 = 340 KB\n",
    "  }\n",
    "\n",
    "  return dict_data \n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "custom_ds = customDataset(path_dataset=path_dataset,\n",
    "                          path_labels=path_labels,\n",
    "                          sample_frame_strategy=sample_frame_strategy,\n",
    "                          stride_window=4,\n",
    "                          preprocess=preprocess,\n",
    "                          clip_length=16)\n",
    "backbone_model = backbone(model_type=model_type)\n",
    "\n",
    "dict_data = _extract_features(dataset=custom_ds,\n",
    "                              path_csv_dataset=path_labels,\n",
    "                              batch_size_feat_extraction=1,\n",
    "                              backbone=backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from custom.helper import CLIPS_REDUCTION,EMBEDDING_REDUCTION,MODEL_TYPE,SAMPLE_FRAME_STRATEGY, HEAD\n",
    "import os\n",
    "from custom.model import Model_Advanced\n",
    "from transformers import AutoImageProcessor\n",
    "from custom.head import HeadSVR, HeadGRU\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_type = MODEL_TYPE.VIDEOMAE_v2_S\n",
    "pooling_embedding_reduction = EMBEDDING_REDUCTION.MEAN_SPATIAL\n",
    "pooling_clips_reduction = CLIPS_REDUCTION.NONE\n",
    "sample_frame_strategy = SAMPLE_FRAME_STRATEGY.SLIDING_WINDOW\n",
    "# path_dict ={\n",
    "#   'all' : os.path.join('partA','starting_point','samples.csv'),\n",
    "  # 'train' : os.path.join('partA','starting_point','train_21.csv'),\n",
    "  # 'val' : os.path.join('partA','starting_point','val_26.csv'),\n",
    "  # 'test' : os.path.join('partA','starting_point','test_5.csv')\n",
    "# }\n",
    "path_dataset = os.path.join('partA','video','video')  \n",
    "path_cvs_dataset = os.path.join('partA','starting_point','samples.csv')\n",
    "head = HEAD.GRU\n",
    "# if head == 'GRU':\n",
    "params = {\n",
    "  'hidden_size': 1024,\n",
    "  'num_layers': 1,\n",
    "  'dropout': 0.0,\n",
    "  'input_size': 768 * 8 # can be 384  (small), 768  (base), 1408  (large) [temporal_dim considered as input sequence for GRU]\n",
    "                    # can be 384*8(small), 768*8(base), 1408*8(large) [temporal_dim considered feature in GRU] \n",
    "}\n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(os.path.join(\"local_model_directory\",\"preprocessor_config.json\"))\n",
    "stride_window_in_video = 16\n",
    "model_advanced = scripts.run_train_test(model_type=model_type, \n",
    "                      pooling_embedding_reduction=pooling_embedding_reduction, \n",
    "                      pooling_clips_reduction=pooling_clips_reduction, \n",
    "                      sample_frame_strategy=sample_frame_strategy, \n",
    "                      path_csv_dataset=path_cvs_dataset, \n",
    "                      path_video_dataset=path_dataset,\n",
    "                      head=head,\n",
    "                      stride_window_in_video=stride_window_in_video, \n",
    "                      head_params=params,\n",
    "                      preprocess=preprocess,\n",
    "                      k_fold = 5,\n",
    "                      epochs = 300,\n",
    "                      train_size=0.8,\n",
    "                      test_size=0.1,\n",
    "                      val_size=0.1,\n",
    "                      batch_size_training=1024,\n",
    "                      batch_size_feat_extraction=8,  \n",
    "                      criterion = nn.L1Loss(),\n",
    "                      optimizer_fn = optim.SGD,\n",
    "                      lr = 0.0001,\n",
    "                      random_state_split_dataset=42,\n",
    "                      only_train=False,\n",
    "                      is_save_features_extracted=False, \n",
    "                      is_validation=True,\n",
    "                      is_plot_dataset_distribution=True,\n",
    "                      is_plot_loss=True,\n",
    "                      is_plot_tsne_backbone_feats=True,\n",
    "                      is_plot_tsne_head_pred=True,\n",
    "                      is_plot_tsne_gru_feats=True,\n",
    "                      is_create_video_prediction=True,\n",
    "                      is_create_video_prediction_per_video=True,\n",
    "                      is_round_output_loss=False,\n",
    "                      is_shuffle_training_batch=True,\n",
    "                      is_shuffle_video_chunks=False,\n",
    "                      is_download_if_unavailable=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import custom.scripts as scripts\n",
    "\n",
    "model_advanced.dataset.stride_window = 40\n",
    "scripts.predict_per_video(\n",
    "    path_csv='partA/starting_point/samples.csv',\n",
    "    sample_ids=[10,35],\n",
    "    model_advanced=model_advanced,\n",
    "    root_folder_path='history_run/VIDEOMAE_v2_B_MEAN_SPATIAL_NONE_SLIDING_WINDOW_GRU_1733007378/train_GRU/video',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_working_directory():\n",
    "  target_dir = 'PainAssessmentVideo'\n",
    "  current_dir = os.getcwd()\n",
    "  if os.path.split(current_dir)[-1] != target_dir:\n",
    "    while os.path.split(current_dir)[-1] != target_dir:\n",
    "      current_dir = os.path.dirname(current_dir)\n",
    "      if current_dir == os.path.dirname(current_dir):  # reached the root directory\n",
    "        raise FileNotFoundError(f\"{target_dir} not found in the directory tree. Please set PainAssessmentVideo as current working directory.\")\n",
    "    os.chdir(current_dir)\n",
    "  print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def create_folder(path):\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(f'Folder created: {path}')\n",
    "  else:\n",
    "    print(f'Folder already exists: {path}')                                                                                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoMaev2_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
